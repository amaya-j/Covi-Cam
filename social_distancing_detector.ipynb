{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"social_distancing_detector.ipynb","provenance":[],"mount_file_id":"1UCf5x3SbaR4pVIqKNg5x3CVLGj3-2PPQ","authorship_tag":"ABX9TyOV1eTrkJcu1fwBonEMYaCa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# ***CoviCAM***\n","\n","## Theory:\n","*   We will be using YOLOv3, trained on COCO dataset for object detection.\n","\n","*   In general, single-stage detectors like YOLO tend to be less accurate than two-stage detectors (R-CNN) but are significantly faster.\n","*   YOLO treats object detection as a regression problem, taking a given input image and simultaneously learning bounding box coordinates and corresponding class label probabilities.\n","\n","*   It is used to return the person prediction probability, bounding box coordinates for the detection, and the centroid of the person.\n","*   NMS (Non-maxima suppression) is also used to reduce overlapping bounding boxes to only a single bounding box, thus representing the true detection of the object. Having overlapping boxes is not exactly practical and ideal, especially if we need to count the number of objects in an image\n","\n","*   Euclidean distance is then computed between all pairs of the returned centroids. Simply, a centroid is the center of a bounding box.\n","*   Based on these pairwise distances, we check to see if any two people are less than/close to 'N' pixels apart\n","\n","\n","\n"],"metadata":{"id":"r5iGVq5sPVqC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6gVDAW0H69V"},"outputs":[],"source":["MIN_CONF=0.3\n","NMS_THRESH=0.3\n","MIN_DISTANCE=50"]},{"cell_type":"markdown","source":["### **Defining a function**\n","\n","*   detects people\n","*   puts bounding boxes\n","\n"],"metadata":{"id":"X58Qa6RoPmk0"}},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","\n","def detect_people( frame, net, ln, personIdx=0):\n","  (H,W)= frame.shape[:2]\n","  results= []\n","  blob= cv2.dnn.blobFromImage(frame, 1/255.0, (416,416), swapRB= True, crop=False)\n","  net.setInput(blob)\n","  layerOutputs= net.forward(ln)\n","\n","  boxes= []\n","  centroids=[]\n","  confidences=[]\n","\n","  for output in layerOutputs:\n","    for detection in output:\n","      scores= detection[5:]\n","      classID= np.argmax(scores)\n","      confidence= scores[classID]\n","\n","      if classID==personIdx and confidence>MIN_CONF:\n","        box= detection[0:4]*np.array([W,H,W,H])\n","        (centerX, centerY, width, height)= box.astype(\"int\")\n","        x= int(centerX-(width/2))\n","        y= int(centerY-(height/2))\n","\n","        boxes.append([x, y, int(width), int(height)])\n","        centroids.append((centerX, centerY))\n","        confidences.append(float(confidence))\n","\n","  idxs= cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n","\n","  if len(idxs)>0:\n","    for i in idxs.flatten():\n","      (x,y)= (boxes[i][0], boxes[i][1])\n","      (w,h)= (boxes[i][2], boxes[i][3])\n","      r= (confidences[i], (x, y, x+w, y+h), centroids[i])\n","      results.append(r)\n","\n","  return results "],"metadata":{"id":"linbLx0pILXA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Processing the video file through the code**\n","*  This may take some time to process.\n","\n","* Kindly look for the output file with the marked violations in Files from Table of Contents after the processing has been completed.\n","\n","* Download the file to view it."],"metadata":{"id":"sjdYbpnKRLIE"}},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","from scipy.spatial import distance as dist\n","import numpy as np\n","import argparse\n","import imutils \n","import cv2\n","import os\n","v=0\n","ap = argparse.ArgumentParser()\n","ap.add_argument(\"-i\", \"--input\", type=str, default=\"\",\n","\thelp=\"path to (optional) input video file\")\n","ap.add_argument(\"-o\", \"--output\", type=str, default=\"\",\n","\thelp=\"path to (optional) output video file\")\n","ap.add_argument(\"-d\", \"--display\", type=int, default=1,\n","\thelp=\"whether or not output frame should be displayed\")\n","args = vars(ap.parse_args([\"--input\",\"/content/drive/MyDrive/CoviCam/footage.mp4\",\"--output\",\"output.avi\",\"--display\",\"1\"]))\n","\n","labelsPath = os.path.sep.join([\"/content/drive/MyDrive/CoviCam/yolo-coco/coco.names\"])\n","LABELS = open(labelsPath).read().strip().split(\"\\n\")\n","\n","weightsPath = os.path.sep.join([\"/content/drive/MyDrive/CoviCam/yolo-coco/yolov3.weights\"])\n","configPath = os.path.sep.join([\"/content/drive/MyDrive/CoviCam/yolo-coco/yolov3.cfg\"])\n","\n","print(\"[INFO] loading YOLO from disk...\")\n","net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n","\n","ln = net.getLayerNames()\n","ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n","\n","print(\"[INFO] accessing video stream...\")\n","vs = cv2.VideoCapture(args[\"input\"] if args[\"input\"] else 0)\n","writer = None\n","\n","while True:\n","\n","\t(grabbed, frame) = vs.read()\n","\n","\tif not grabbed:\n","\t\tbreak\n","\n","\tframe = imutils.resize(frame, width=700)\n","\tresults = detect_people(frame, net, ln,\n","\t\tpersonIdx=LABELS.index(\"person\"))\n","\n","\tviolate = set()\n","\n","\tif len(results) >= 2:\n","\n","\t\tcentroids = np.array([r[2] for r in results])\n","\t\tD = dist.cdist(centroids, centroids, metric=\"euclidean\")\n","\n","\t\tfor i in range(0, D.shape[0]):\n","\t\t\tfor j in range(i + 1, D.shape[1]):\n","\n","\t\t\t\tif D[i, j] < MIN_DISTANCE:\n","          \n","\t\t\t\t\tviolate.add(i)\n","\t\t\t\t\tviolate.add(j)\n","\n","\tfor (i, (prob, bbox, centroid)) in enumerate(results):\n","\n","\t\t(startX, startY, endX, endY) = bbox\n","\t\t(cX, cY) = centroid\n","\t\tcolor = (0, 255, 0)\n","\n","\t\tif i in violate:\n","\t\t\tcolor = (0, 0, 255)\n","\n","\n","\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n","\t\tcv2.circle(frame, (cX, cY), 5, color, 1)\n","  \n","\tcv2.rectangle(frame,(5,10),(700,30),(255,255,255),40)\n"," \n","\ttext = \"Social Distancing Violations: {}\".format(len(violate))\n","\tcv2.putText(frame, text, (0, frame.shape[0]-360),\n","\t\tcv2.FONT_HERSHEY_COMPLEX, 0.65, (95,26,73), 2)\n","\n","\t \n","\tif args[\"display\"] > 0:\n","\n","\t\tcv2_imshow(frame)\n","\t\tkey = cv2.waitKey(1) & 0xFF\n","\n","\t\tif key == ord(\"q\"):\n","\t\t\tbreak\n","\n","\tif args[\"output\"] != \"\" and writer is None:\n","\t\n","\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n","\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 25,\n","\t\t\t(frame.shape[1], frame.shape[0]), True)\n","\n","\tif writer is not None:\n","\t\twriter.write(frame)\n","\t\n","\n"],"metadata":{"id":"ebVZnRicPCnM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# THANK YOU!"],"metadata":{"id":"hrk1vXzlSO0k"}}]}